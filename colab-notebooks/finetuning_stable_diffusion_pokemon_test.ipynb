{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMSBRcs7C8OLRP3hG7unCH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBYLdqslwrCh",
        "outputId": "b07b3192-11f8-4d67-e036-18eeb3bb9fef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 10 01:42:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying to use diffusers library directly\n",
        "* Would need 32GB RAM for GPU --> Select A100 GPUs on Colab"
      ],
      "metadata": {
        "id": "UYGuGvbcI7ty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vonfbmLl1lW",
        "outputId": "8c7b50de-af73-4911-bb28-f92211c78080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 25836, done.\u001b[K\n",
            "remote: Counting objects: 100% (11558/11558), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1026/1026), done.\u001b[K\n",
            "remote: Total 25836 (delta 10976), reused 10598 (delta 10508), pack-reused 14278\u001b[K\n",
            "Receiving objects: 100% (25836/25836), 31.60 MiB | 20.31 MiB/s, done.\n",
            "Resolving deltas: 100% (18945/18945), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/diffusers\n",
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrddqCi_qOt8",
        "outputId": "3fe56e3e-57c9-4237-9363-caa573f127c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusers\n",
            "CITATION.cff\t    docs      MANIFEST.in     scripts\t tests\n",
            "CODE_OF_CONDUCT.md  examples  PHILOSOPHY.md   setup.cfg  _typos.toml\n",
            "CONTRIBUTING.md     LICENSE   pyproject.toml  setup.py\t utils\n",
            "docker\t\t    Makefile  README.md       src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1KiSeQEqP-S",
        "outputId": "cb88394f-09f5-4b8c-f003-48cee0dd593a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata (from diffusers==0.17.0.dev0)\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (3.12.0)\n",
            "Collecting huggingface-hub>=0.13.2 (from diffusers==0.17.0.dev0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (8.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (23.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.17.0.dev0) (3.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.17.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.17.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.17.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.17.0.dev0) (3.4)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.17.0.dev0-py3-none-any.whl size=965525 sha256=920400022067b5c38fa17f35d866d76a1d4938b20dbd3e35891dc59716bfa96a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lyjwlkyt/wheels/95/c5/3b/e1b4269f8a2584de57e75f949a185b48fc4144e9a91fc9965a\n",
            "Successfully built diffusers\n",
            "Installing collected packages: importlib-metadata, huggingface-hub, diffusers\n",
            "Successfully installed diffusers-0.17.0.dev0 huggingface-hub-0.14.1 importlib-metadata-6.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/diffusers/examples/text_to_image\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nxgFRzamEeF",
        "outputId": "cf4f4bac-5cc8-4d99-b4bc-dfaef378e71a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffusers/examples/text_to_image\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate>=0.16.0 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.15.1+cu118)\n",
            "Collecting transformers>=4.25.1 (from -r requirements.txt (line 3))\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from -r requirements.txt (line 4))\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ftfy (from -r requirements.txt (line 5))\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.12.2)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (16.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->-r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (1.5.3)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 4)) (2023.4.0)\n",
            "Collecting aiohttp (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets->-r requirements.txt (line 4))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->-r requirements.txt (line 5)) (0.2.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.40.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->-r requirements.txt (line 7)) (2.1.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 4)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 4))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 6)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: tokenizers, xxhash, multidict, ftfy, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, transformers, aiohttp, datasets, accelerate\n",
            "Successfully installed accelerate-0.19.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 ftfy-6.1.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-uImfSvnyqb",
        "outputId": "aa5c059c-fc9f-4d2b-f8cd-a4ffc68be48a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.22.2-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=1d145e2c68a04eb6c7d145ef34de5f53f1ecee63fa9e9d07476a7e36647d676e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.22.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq4P7lh-UPbb",
        "outputId": "5b2b7317-0dea-4e2f-f406-98e22daef241"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.19-cp310-cp310-manylinux2014_x86_64.whl (108.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.2/108.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.0+cu118)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->xformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->xformers) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->xformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->xformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, xformers\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.29 typing-inspect-0.8.0 xformers-0.0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "PThqkq6_rPzv",
        "outputId": "7d69cea7-35ff-48e0-ceee-e41e30128237"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/74962932/not-able-to-select-an-option-on-google-colab\n",
        "!accelerate config default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK8ac-NdmZFP",
        "outputId": "1c0d2eac-62ea-459d-fa52-30f946fe5776"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-10 01:44:05.178844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "IqFyQ1Zws1zd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBADYBSFmdCM",
        "outputId": "ac916bc4-a149-4285-8a95-d7f72a6cc84b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LORA training script\n",
        "# note: resolution is 768 for stable-diffusion-2-1 model\n",
        "%%bash\n",
        "accelerate launch --mixed_precision=\"fp16\" train_text_to_image_lora.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" \\\n",
        "  --dataset_name=\"lambdalabs/pokemon-blip-captions\" --caption_column=\"text\" \\\n",
        "  --resolution=768 --random_flip \\\n",
        "  --train_batch_size=1 \\\n",
        "  --num_train_epochs=100 --checkpointing_steps=5000 \\\n",
        "  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
        "  --seed=42 \\\n",
        "  --output_dir=\"sd-pokemon-model-lora\" \\\n",
        "  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n",
        "  --push_to_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Dyrc8VXm15x",
        "outputId": "de4ef1f1-4795-412e-d480-2f4ace2f219d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset imagefolder/pokemon (download: 95.05 MiB, generated: 113.89 MiB, post-processed: Unknown size, total: 208.94 MiB) to /root/.cache/huggingface/datasets/lambdalabs___parquet/lambdalabs--pokemon-blip-captions-10e3527a764857bd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/lambdalabs___parquet/lambdalabs--pokemon-blip-captions-10e3527a764857bd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-05-09 23:51:01.641921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-09 23:51:07.672046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:258: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of ğŸ¤— Accelerate. Use `project_dir` instead.\n",
            "  warnings.warn(\n",
            "05/09/2023 23:51:10 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "\rDownloading (â€¦)cheduler_config.json:   0%|          | 0.00/345 [00:00<?, ?B/s]\rDownloading (â€¦)cheduler_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 345/345 [00:00<00:00, 2.34MB/s]\n",
            "{'thresholding', 'dynamic_thresholding_ratio', 'clip_sample_range', 'variance_type', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
            "\rDownloading (â€¦)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]\rDownloading (â€¦)tokenizer/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.06M/1.06M [00:00<00:00, 8.70MB/s]\rDownloading (â€¦)tokenizer/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.06M/1.06M [00:00<00:00, 8.58MB/s]\n",
            "\rDownloading (â€¦)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]\rDownloading (â€¦)tokenizer/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 525k/525k [00:00<00:00, 23.0MB/s]\n",
            "\rDownloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]\rDownloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460/460 [00:00<00:00, 3.50MB/s]\n",
            "\rDownloading (â€¦)okenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]\rDownloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 824/824 [00:00<00:00, 4.85MB/s]\n",
            "\rDownloading (â€¦)_encoder/config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]\rDownloading (â€¦)_encoder/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 633/633 [00:00<00:00, 3.72MB/s]\n",
            "\rDownloading pytorch_model.bin:   0%|          | 0.00/1.36G [00:00<?, ?B/s]\rDownloading pytorch_model.bin:   1%|          | 10.5M/1.36G [00:00<01:25, 15.8MB/s]\rDownloading pytorch_model.bin:   2%|â–         | 21.0M/1.36G [00:01<01:23, 16.0MB/s]\rDownloading pytorch_model.bin:   2%|â–         | 31.5M/1.36G [00:01<01:23, 15.9MB/s]\rDownloading pytorch_model.bin:   3%|â–         | 41.9M/1.36G [00:02<01:22, 16.1MB/s]\rDownloading pytorch_model.bin:   4%|â–         | 52.4M/1.36G [00:03<01:21, 16.1MB/s]\rDownloading pytorch_model.bin:   5%|â–         | 62.9M/1.36G [00:03<01:20, 16.1MB/s]\rDownloading pytorch_model.bin:   5%|â–Œ         | 73.4M/1.36G [00:04<01:19, 16.2MB/s]\rDownloading pytorch_model.bin:   6%|â–Œ         | 83.9M/1.36G [00:05<01:18, 16.2MB/s]\rDownloading pytorch_model.bin:   7%|â–‹         | 94.4M/1.36G [00:05<01:18, 16.2MB/s]\rDownloading pytorch_model.bin:   8%|â–Š         | 105M/1.36G [00:06<01:17, 16.2MB/s] \rDownloading pytorch_model.bin:   8%|â–Š         | 115M/1.36G [00:07<01:17, 16.2MB/s]\rDownloading pytorch_model.bin:   9%|â–‰         | 126M/1.36G [00:07<01:16, 16.2MB/s]\rDownloading pytorch_model.bin:  10%|â–ˆ         | 136M/1.36G [00:08<01:15, 16.2MB/s]\rDownloading pytorch_model.bin:  11%|â–ˆ         | 147M/1.36G [00:09<01:15, 16.2MB/s]\rDownloading pytorch_model.bin:  12%|â–ˆâ–        | 157M/1.36G [00:09<01:14, 16.2MB/s]\rDownloading pytorch_model.bin:  12%|â–ˆâ–        | 168M/1.36G [00:10<01:13, 16.2MB/s]\rDownloading pytorch_model.bin:  13%|â–ˆâ–        | 178M/1.36G [00:11<01:12, 16.3MB/s]\rDownloading pytorch_model.bin:  14%|â–ˆâ–        | 189M/1.36G [00:11<01:12, 16.3MB/s]\rDownloading pytorch_model.bin:  15%|â–ˆâ–        | 199M/1.36G [00:12<01:11, 16.2MB/s]\rDownloading pytorch_model.bin:  15%|â–ˆâ–Œ        | 210M/1.36G [00:13<01:15, 15.2MB/s]\rDownloading pytorch_model.bin:  16%|â–ˆâ–Œ        | 220M/1.36G [00:13<01:13, 15.5MB/s]\rDownloading pytorch_model.bin:  17%|â–ˆâ–‹        | 231M/1.36G [00:14<01:11, 15.7MB/s]\rDownloading pytorch_model.bin:  18%|â–ˆâ–Š        | 241M/1.36G [00:15<01:10, 15.9MB/s]\rDownloading pytorch_model.bin:  18%|â–ˆâ–Š        | 252M/1.36G [00:15<01:09, 16.0MB/s]\rDownloading pytorch_model.bin:  19%|â–ˆâ–‰        | 262M/1.36G [00:16<01:08, 16.0MB/s]\rDownloading pytorch_model.bin:  20%|â–ˆâ–ˆ        | 273M/1.36G [00:16<01:07, 16.1MB/s]\rDownloading pytorch_model.bin:  21%|â–ˆâ–ˆ        | 283M/1.36G [00:17<01:06, 16.1MB/s]\rDownloading pytorch_model.bin:  22%|â–ˆâ–ˆâ–       | 294M/1.36G [00:18<01:06, 16.2MB/s]\rDownloading pytorch_model.bin:  22%|â–ˆâ–ˆâ–       | 304M/1.36G [00:18<01:05, 16.2MB/s]\rDownloading pytorch_model.bin:  23%|â–ˆâ–ˆâ–       | 315M/1.36G [00:19<01:04, 16.2MB/s]\rDownloading pytorch_model.bin:  24%|â–ˆâ–ˆâ–       | 325M/1.36G [00:20<01:03, 16.2MB/s]\rDownloading pytorch_model.bin:  25%|â–ˆâ–ˆâ–       | 336M/1.36G [00:20<01:03, 16.3MB/s]\rDownloading pytorch_model.bin:  25%|â–ˆâ–ˆâ–Œ       | 346M/1.36G [00:21<01:02, 16.3MB/s]\rDownloading pytorch_model.bin:  26%|â–ˆâ–ˆâ–Œ       | 357M/1.36G [00:22<01:01, 16.3MB/s]\rDownloading pytorch_model.bin:  27%|â–ˆâ–ˆâ–‹       | 367M/1.36G [00:22<01:01, 16.3MB/s]\rDownloading pytorch_model.bin:  28%|â–ˆâ–ˆâ–Š       | 377M/1.36G [00:23<01:00, 16.3MB/s]\rDownloading pytorch_model.bin:  28%|â–ˆâ–ˆâ–Š       | 388M/1.36G [00:24<00:59, 16.3MB/s]\rDownloading pytorch_model.bin:  29%|â–ˆâ–ˆâ–‰       | 398M/1.36G [00:24<00:59, 16.2MB/s]\rDownloading pytorch_model.bin:  30%|â–ˆâ–ˆâ–ˆ       | 409M/1.36G [00:25<00:58, 16.2MB/s]\rDownloading pytorch_model.bin:  31%|â–ˆâ–ˆâ–ˆ       | 419M/1.36G [00:26<00:58, 16.2MB/s]\rDownloading pytorch_model.bin:  32%|â–ˆâ–ˆâ–ˆâ–      | 430M/1.36G [00:26<00:57, 16.2MB/s]\rDownloading pytorch_model.bin:  32%|â–ˆâ–ˆâ–ˆâ–      | 440M/1.36G [00:27<00:56, 16.2MB/s]\rDownloading pytorch_model.bin:  33%|â–ˆâ–ˆâ–ˆâ–      | 451M/1.36G [00:27<00:56, 16.2MB/s]\rDownloading pytorch_model.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 461M/1.36G [00:28<00:55, 16.2MB/s]\rDownloading pytorch_model.bin:  35%|â–ˆâ–ˆâ–ˆâ–      | 472M/1.36G [00:29<00:55, 16.1MB/s]\rDownloading pytorch_model.bin:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 482M/1.36G [00:29<00:54, 16.2MB/s]\rDownloading pytorch_model.bin:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 493M/1.36G [00:30<00:53, 16.2MB/s]\rDownloading pytorch_model.bin:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 503M/1.36G [00:31<00:52, 16.2MB/s]\rDownloading pytorch_model.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 514M/1.36G [00:31<00:52, 16.2MB/s]\rDownloading pytorch_model.bin:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 524M/1.36G [00:32<00:55, 15.1MB/s]\rDownloading pytorch_model.bin:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 535M/1.36G [00:33<00:49, 16.6MB/s]\rDownloading pytorch_model.bin:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 545M/1.36G [00:33<00:49, 16.4MB/s]\rDownloading pytorch_model.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 556M/1.36G [00:34<00:49, 16.3MB/s]\rDownloading pytorch_model.bin:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 566M/1.36G [00:35<00:48, 16.3MB/s]\rDownloading pytorch_model.bin:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 577M/1.36G [00:35<00:48, 16.3MB/s]\rDownloading pytorch_model.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 587M/1.36G [00:36<00:47, 16.3MB/s]\rDownloading pytorch_model.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 598M/1.36G [00:37<00:47, 16.2MB/s]\rDownloading pytorch_model.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 608M/1.36G [00:37<00:46, 16.2MB/s]\rDownloading pytorch_model.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 619M/1.36G [00:38<00:45, 16.2MB/s]\rDownloading pytorch_model.bin:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 629M/1.36G [00:38<00:45, 16.3MB/s]\rDownloading pytorch_model.bin:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 640M/1.36G [00:39<00:44, 16.2MB/s]\rDownloading pytorch_model.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 650M/1.36G [00:40<00:43, 16.2MB/s]\rDownloading pytorch_model.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 661M/1.36G [00:40<00:43, 16.3MB/s]\rDownloading pytorch_model.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 671M/1.36G [00:41<00:42, 16.3MB/s]\rDownloading pytorch_model.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 682M/1.36G [00:42<00:41, 16.2MB/s]\rDownloading pytorch_model.bin:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 692M/1.36G [00:42<00:41, 16.2MB/s]\rDownloading pytorch_model.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 703M/1.36G [00:43<00:40, 16.2MB/s]\rDownloading pytorch_model.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 713M/1.36G [00:44<00:39, 16.2MB/s]\rDownloading pytorch_model.bin:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 724M/1.36G [00:44<00:39, 16.3MB/s]\rDownloading pytorch_model.bin:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 734M/1.36G [00:45<00:38, 16.2MB/s]\rDownloading pytorch_model.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 744M/1.36G [00:46<00:38, 16.2MB/s]\rDownloading pytorch_model.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 755M/1.36G [00:46<00:37, 16.2MB/s]\rDownloading pytorch_model.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 765M/1.36G [00:47<00:36, 16.2MB/s]\rDownloading pytorch_model.bin:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 776M/1.36G [00:48<00:36, 16.2MB/s]\rDownloading pytorch_model.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 786M/1.36G [00:48<00:35, 16.2MB/s]\rDownloading pytorch_model.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 797M/1.36G [00:49<00:34, 16.2MB/s]\rDownloading pytorch_model.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 807M/1.36G [00:49<00:34, 16.2MB/s]\rDownloading pytorch_model.bin:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 818M/1.36G [00:50<00:33, 16.2MB/s]\rDownloading pytorch_model.bin:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 828M/1.36G [00:51<00:32, 16.2MB/s]\rDownloading pytorch_model.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 839M/1.36G [00:51<00:32, 16.2MB/s]\rDownloading pytorch_model.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 849M/1.36G [00:52<00:31, 16.2MB/s]\rDownloading pytorch_model.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 860M/1.36G [00:53<00:30, 16.2MB/s]\rDownloading pytorch_model.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 870M/1.36G [00:53<00:30, 16.2MB/s]\rDownloading pytorch_model.bin:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 881M/1.36G [00:54<00:29, 16.2MB/s]\rDownloading pytorch_model.bin:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 891M/1.36G [00:55<00:28, 16.3MB/s]\rDownloading pytorch_model.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 902M/1.36G [00:55<00:28, 16.2MB/s]\rDownloading pytorch_model.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 912M/1.36G [00:56<00:27, 16.2MB/s]\rDownloading pytorch_model.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 923M/1.36G [00:57<00:27, 16.2MB/s]\rDownloading pytorch_model.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 933M/1.36G [00:57<00:26, 16.2MB/s]\rDownloading pytorch_model.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 944M/1.36G [00:58<00:25, 16.3MB/s]\rDownloading pytorch_model.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 954M/1.36G [00:59<00:25, 16.1MB/s]\rDownloading pytorch_model.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 965M/1.36G [00:59<00:24, 16.1MB/s]\rDownloading pytorch_model.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 975M/1.36G [01:00<00:23, 16.1MB/s]\rDownloading pytorch_model.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 986M/1.36G [01:00<00:23, 16.2MB/s]\rDownloading pytorch_model.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 996M/1.36G [01:01<00:22, 16.2MB/s]\rDownloading pytorch_model.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.01G/1.36G [01:02<00:21, 16.2MB/s]\rDownloading pytorch_model.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.02G/1.36G [01:02<00:21, 16.2MB/s]\rDownloading pytorch_model.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1.03G/1.36G [01:03<00:20, 16.2MB/s]\rDownloading pytorch_model.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1.04G/1.36G [01:04<00:19, 16.2MB/s]\rDownloading pytorch_model.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1.05G/1.36G [01:04<00:19, 16.2MB/s]\rDownloading pytorch_model.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1.06G/1.36G [01:05<00:18, 16.2MB/s]\rDownloading pytorch_model.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1.07G/1.36G [01:06<00:18, 16.2MB/s]\rDownloading pytorch_model.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1.08G/1.36G [01:06<00:17, 16.2MB/s]\rDownloading pytorch_model.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1.09G/1.36G [01:07<00:16, 16.2MB/s]\rDownloading pytorch_model.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1.10G/1.36G [01:08<00:16, 16.2MB/s]\rDownloading pytorch_model.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.11G/1.36G [01:08<00:15, 16.0MB/s]\rDownloading pytorch_model.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.12G/1.36G [01:09<00:14, 16.1MB/s]\rDownloading pytorch_model.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.13G/1.36G [01:10<00:14, 16.1MB/s]\rDownloading pytorch_model.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.14G/1.36G [01:10<00:13, 16.2MB/s]\rDownloading pytorch_model.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.15G/1.36G [01:11<00:12, 16.2MB/s]\rDownloading pytorch_model.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1.16G/1.36G [01:11<00:12, 16.1MB/s]\rDownloading pytorch_model.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1.17G/1.36G [01:12<00:11, 16.2MB/s]\rDownloading pytorch_model.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1.18G/1.36G [01:13<00:10, 16.2MB/s]\rDownloading pytorch_model.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1.20G/1.36G [01:13<00:10, 16.2MB/s]\rDownloading pytorch_model.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1.21G/1.36G [01:14<00:09, 16.2MB/s]\rDownloading pytorch_model.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1.22G/1.36G [01:15<00:08, 16.2MB/s]\rDownloading pytorch_model.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1.23G/1.36G [01:15<00:08, 16.2MB/s]\rDownloading pytorch_model.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1.24G/1.36G [01:16<00:07, 16.2MB/s]\rDownloading pytorch_model.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.25G/1.36G [01:17<00:07, 16.2MB/s]\rDownloading pytorch_model.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.26G/1.36G [01:17<00:06, 16.2MB/s]\rDownloading pytorch_model.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.27G/1.36G [01:18<00:05, 16.2MB/s]\rDownloading pytorch_model.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.28G/1.36G [01:19<00:05, 16.2MB/s]\rDownloading pytorch_model.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.29G/1.36G [01:19<00:04, 16.2MB/s]\rDownloading pytorch_model.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1.30G/1.36G [01:20<00:03, 16.2MB/s]\rDownloading pytorch_model.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1.31G/1.36G [01:21<00:03, 16.2MB/s]\rDownloading pytorch_model.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1.32G/1.36G [01:21<00:02, 16.2MB/s]\rDownloading pytorch_model.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1.33G/1.36G [01:22<00:01, 16.2MB/s]\rDownloading pytorch_model.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1.34G/1.36G [01:22<00:01, 16.2MB/s]\rDownloading pytorch_model.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1.35G/1.36G [01:23<00:00, 16.2MB/s]\rDownloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36G/1.36G [01:24<00:00, 16.1MB/s]\rDownloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36G/1.36G [01:24<00:00, 16.2MB/s]\n",
            "\rDownloading (â€¦)main/vae/config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]\rDownloading (â€¦)main/vae/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 611/611 [00:00<00:00, 3.39MB/s]\n",
            "\rDownloading (â€¦)on_pytorch_model.bin:   0%|          | 0.00/335M [00:00<?, ?B/s]\rDownloading (â€¦)on_pytorch_model.bin:   3%|â–         | 10.5M/335M [00:00<00:19, 16.6MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   6%|â–‹         | 21.0M/335M [00:01<00:19, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   9%|â–‰         | 31.5M/335M [00:01<00:18, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  13%|â–ˆâ–        | 41.9M/335M [00:02<00:17, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  16%|â–ˆâ–Œ        | 52.4M/335M [00:03<00:17, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  19%|â–ˆâ–‰        | 62.9M/335M [00:03<00:16, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  22%|â–ˆâ–ˆâ–       | 73.4M/335M [00:04<00:16, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  25%|â–ˆâ–ˆâ–Œ       | 83.9M/335M [00:05<00:15, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  28%|â–ˆâ–ˆâ–Š       | 94.4M/335M [00:05<00:14, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  31%|â–ˆâ–ˆâ–ˆâ–      | 105M/335M [00:06<00:14, 16.3MB/s] \rDownloading (â€¦)on_pytorch_model.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 115M/335M [00:07<00:13, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 126M/335M [00:07<00:12, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 136M/335M [00:08<00:12, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147M/335M [00:09<00:11, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 157M/335M [00:09<00:10, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 168M/335M [00:10<00:10, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 178M/335M [00:10<00:09, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 189M/335M [00:11<00:09, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 199M/335M [00:12<00:08, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 210M/335M [00:12<00:07, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 220M/335M [00:13<00:07, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 231M/335M [00:14<00:06, 15.9MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 241M/335M [00:14<00:05, 15.9MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 252M/335M [00:15<00:05, 16.0MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 262M/335M [00:16<00:04, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 273M/335M [00:16<00:03, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 283M/335M [00:17<00:03, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 294M/335M [00:18<00:02, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 304M/335M [00:18<00:01, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 315M/335M [00:19<00:01, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 325M/335M [00:20<00:00, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:20<00:00, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 335M/335M [00:20<00:00, 16.2MB/s]\n",
            "{'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "\rDownloading (â€¦)ain/unet/config.json:   0%|          | 0.00/939 [00:00<?, ?B/s]\rDownloading (â€¦)ain/unet/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 939/939 [00:00<00:00, 4.85MB/s]\n",
            "\rDownloading (â€¦)on_pytorch_model.bin:   0%|          | 0.00/3.46G [00:00<?, ?B/s]\rDownloading (â€¦)on_pytorch_model.bin:   0%|          | 10.5M/3.46G [00:00<03:27, 16.6MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   1%|          | 21.0M/3.46G [00:01<03:29, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   1%|          | 31.5M/3.46G [00:01<03:31, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   1%|          | 41.9M/3.46G [00:02<03:30, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   2%|â–         | 52.4M/3.46G [00:03<03:30, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   2%|â–         | 62.9M/3.46G [00:03<03:29, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   2%|â–         | 73.4M/3.46G [00:04<03:28, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   2%|â–         | 83.9M/3.46G [00:05<03:28, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   3%|â–         | 94.4M/3.46G [00:05<03:28, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   3%|â–         | 105M/3.46G [00:06<03:27, 16.2MB/s] \rDownloading (â€¦)on_pytorch_model.bin:   3%|â–         | 115M/3.46G [00:07<03:29, 16.0MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   4%|â–         | 126M/3.46G [00:07<03:27, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   4%|â–         | 136M/3.46G [00:08<03:27, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   4%|â–         | 147M/3.46G [00:09<03:25, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   5%|â–         | 157M/3.46G [00:09<03:25, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   5%|â–         | 168M/3.46G [00:10<03:24, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   5%|â–Œ         | 178M/3.46G [00:11<03:23, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   5%|â–Œ         | 189M/3.46G [00:11<03:22, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   6%|â–Œ         | 199M/3.46G [00:12<03:21, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   6%|â–Œ         | 210M/3.46G [00:12<03:21, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   6%|â–‹         | 220M/3.46G [00:13<03:20, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   7%|â–‹         | 231M/3.46G [00:14<03:19, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   7%|â–‹         | 241M/3.46G [00:15<03:36, 14.9MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   7%|â–‹         | 252M/3.46G [00:15<03:13, 16.6MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   8%|â–Š         | 262M/3.46G [00:16<03:13, 16.5MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   8%|â–Š         | 273M/3.46G [00:16<03:14, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   8%|â–Š         | 283M/3.46G [00:17<03:14, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   8%|â–Š         | 294M/3.46G [00:18<03:14, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   9%|â–‰         | 304M/3.46G [00:18<03:14, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   9%|â–‰         | 315M/3.46G [00:19<03:14, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:   9%|â–‰         | 325M/3.46G [00:20<03:13, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  10%|â–‰         | 336M/3.46G [00:20<03:12, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  10%|â–‰         | 346M/3.46G [00:21<03:12, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  10%|â–ˆ         | 357M/3.46G [00:22<03:11, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  11%|â–ˆ         | 367M/3.46G [00:22<03:10, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  11%|â–ˆ         | 377M/3.46G [00:23<03:10, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  11%|â–ˆ         | 388M/3.46G [00:23<03:09, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  12%|â–ˆâ–        | 398M/3.46G [00:24<03:08, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  12%|â–ˆâ–        | 409M/3.46G [00:25<03:08, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  12%|â–ˆâ–        | 419M/3.46G [00:25<03:07, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  12%|â–ˆâ–        | 430M/3.46G [00:26<03:07, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  13%|â–ˆâ–        | 440M/3.46G [00:27<03:06, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  13%|â–ˆâ–        | 451M/3.46G [00:27<03:05, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  13%|â–ˆâ–        | 461M/3.46G [00:28<03:04, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  14%|â–ˆâ–        | 472M/3.46G [00:29<03:04, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  14%|â–ˆâ–        | 482M/3.46G [00:29<03:03, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  14%|â–ˆâ–        | 493M/3.46G [00:30<03:03, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  15%|â–ˆâ–        | 503M/3.46G [00:31<03:02, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  15%|â–ˆâ–        | 514M/3.46G [00:31<03:01, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  15%|â–ˆâ–Œ        | 524M/3.46G [00:32<03:05, 15.8MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  15%|â–ˆâ–Œ        | 535M/3.46G [00:33<02:58, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  16%|â–ˆâ–Œ        | 545M/3.46G [00:33<02:58, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  16%|â–ˆâ–Œ        | 556M/3.46G [00:34<02:58, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  16%|â–ˆâ–‹        | 566M/3.46G [00:34<02:58, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  17%|â–ˆâ–‹        | 577M/3.46G [00:35<02:58, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  17%|â–ˆâ–‹        | 587M/3.46G [00:36<02:57, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  17%|â–ˆâ–‹        | 598M/3.46G [00:36<02:56, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  18%|â–ˆâ–Š        | 608M/3.46G [00:37<02:55, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  18%|â–ˆâ–Š        | 619M/3.46G [00:38<02:55, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  18%|â–ˆâ–Š        | 629M/3.46G [00:38<02:54, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  18%|â–ˆâ–Š        | 640M/3.46G [00:39<02:53, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  19%|â–ˆâ–‰        | 650M/3.46G [00:40<02:53, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  19%|â–ˆâ–‰        | 661M/3.46G [00:40<02:52, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  19%|â–ˆâ–‰        | 671M/3.46G [00:41<02:51, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  20%|â–ˆâ–‰        | 682M/3.46G [00:42<02:51, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  20%|â–ˆâ–‰        | 692M/3.46G [00:42<02:51, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  20%|â–ˆâ–ˆ        | 703M/3.46G [00:43<02:50, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  21%|â–ˆâ–ˆ        | 713M/3.46G [00:43<02:49, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  21%|â–ˆâ–ˆ        | 724M/3.46G [00:44<02:48, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  21%|â–ˆâ–ˆ        | 734M/3.46G [00:45<02:47, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  21%|â–ˆâ–ˆâ–       | 744M/3.46G [00:45<02:47, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  22%|â–ˆâ–ˆâ–       | 755M/3.46G [00:46<02:47, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  22%|â–ˆâ–ˆâ–       | 765M/3.46G [00:47<02:47, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  22%|â–ˆâ–ˆâ–       | 776M/3.46G [00:47<02:46, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  23%|â–ˆâ–ˆâ–       | 786M/3.46G [00:48<02:45, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  23%|â–ˆâ–ˆâ–       | 797M/3.46G [00:49<02:45, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  23%|â–ˆâ–ˆâ–       | 807M/3.46G [00:49<02:44, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  24%|â–ˆâ–ˆâ–       | 818M/3.46G [00:50<02:43, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  24%|â–ˆâ–ˆâ–       | 828M/3.46G [00:51<02:43, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  24%|â–ˆâ–ˆâ–       | 839M/3.46G [00:51<02:41, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  25%|â–ˆâ–ˆâ–       | 849M/3.46G [00:52<02:41, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  25%|â–ˆâ–ˆâ–       | 860M/3.46G [00:53<02:40, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  25%|â–ˆâ–ˆâ–Œ       | 870M/3.46G [00:53<02:40, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  25%|â–ˆâ–ˆâ–Œ       | 881M/3.46G [00:54<02:50, 15.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  26%|â–ˆâ–ˆâ–Œ       | 891M/3.46G [00:55<02:36, 16.5MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  26%|â–ˆâ–ˆâ–Œ       | 902M/3.46G [00:55<02:36, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  26%|â–ˆâ–ˆâ–‹       | 912M/3.46G [00:56<02:36, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  27%|â–ˆâ–ˆâ–‹       | 923M/3.46G [00:56<02:35, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  27%|â–ˆâ–ˆâ–‹       | 933M/3.46G [00:57<02:35, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  27%|â–ˆâ–ˆâ–‹       | 944M/3.46G [00:58<02:34, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  28%|â–ˆâ–ˆâ–Š       | 954M/3.46G [00:58<02:35, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  28%|â–ˆâ–ˆâ–Š       | 965M/3.46G [00:59<02:34, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  28%|â–ˆâ–ˆâ–Š       | 975M/3.46G [01:00<02:33, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  28%|â–ˆâ–ˆâ–Š       | 986M/3.46G [01:00<02:33, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  29%|â–ˆâ–ˆâ–‰       | 996M/3.46G [01:01<02:32, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  29%|â–ˆâ–ˆâ–‰       | 1.01G/3.46G [01:02<02:31, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  29%|â–ˆâ–ˆâ–‰       | 1.02G/3.46G [01:02<02:31, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  30%|â–ˆâ–ˆâ–‰       | 1.03G/3.46G [01:03<02:31, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  30%|â–ˆâ–ˆâ–‰       | 1.04G/3.46G [01:04<02:30, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  30%|â–ˆâ–ˆâ–ˆ       | 1.05G/3.46G [01:04<02:29, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  31%|â–ˆâ–ˆâ–ˆ       | 1.06G/3.46G [01:05<02:29, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  31%|â–ˆâ–ˆâ–ˆ       | 1.07G/3.46G [01:06<02:28, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  31%|â–ˆâ–ˆâ–ˆ       | 1.08G/3.46G [01:06<02:27, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  31%|â–ˆâ–ˆâ–ˆâ–      | 1.09G/3.46G [01:07<02:33, 15.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.10G/3.46G [01:08<02:24, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.11G/3.46G [01:08<02:24, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  32%|â–ˆâ–ˆâ–ˆâ–      | 1.12G/3.46G [01:09<02:23, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  33%|â–ˆâ–ˆâ–ˆâ–      | 1.13G/3.46G [01:09<02:23, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  33%|â–ˆâ–ˆâ–ˆâ–      | 1.14G/3.46G [01:10<02:23, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  33%|â–ˆâ–ˆâ–ˆâ–      | 1.15G/3.46G [01:11<02:22, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 1.16G/3.46G [01:11<02:21, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 1.17G/3.46G [01:12<02:21, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 1.18G/3.46G [01:13<02:20, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  35%|â–ˆâ–ˆâ–ˆâ–      | 1.20G/3.46G [01:13<02:19, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  35%|â–ˆâ–ˆâ–ˆâ–      | 1.21G/3.46G [01:14<02:19, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.22G/3.46G [01:15<02:18, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.23G/3.46G [01:15<02:18, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.24G/3.46G [01:16<02:17, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1.25G/3.46G [01:17<02:17, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 1.26G/3.46G [01:17<02:16, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.27G/3.46G [01:18<02:15, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.28G/3.46G [01:19<02:14, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 1.29G/3.46G [01:19<02:14, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.30G/3.46G [01:20<02:13, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.31G/3.46G [01:20<02:13, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.32G/3.46G [01:21<02:12, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1.33G/3.46G [01:22<02:11, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 1.34G/3.46G [01:22<02:11, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1.35G/3.46G [01:23<02:10, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 1.36G/3.46G [01:24<02:09, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1.37G/3.46G [01:24<02:09, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 1.38G/3.46G [01:25<02:08, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.39G/3.46G [01:26<02:07, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.41G/3.46G [01:26<02:07, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.42G/3.46G [01:27<02:06, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1.43G/3.46G [01:28<02:05, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.44G/3.46G [01:28<02:05, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.45G/3.46G [01:29<02:04, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.46G/3.46G [01:30<02:03, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.47G/3.46G [01:30<02:03, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.48G/3.46G [01:31<02:02, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.49G/3.46G [01:31<02:01, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.50G/3.46G [01:32<02:01, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.51G/3.46G [01:33<02:00, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.52G/3.46G [01:33<01:59, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.53G/3.46G [01:34<01:59, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.54G/3.46G [01:35<01:58, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.55G/3.46G [01:35<01:58, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.56G/3.46G [01:36<01:57, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.57G/3.46G [01:37<01:56, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.58G/3.46G [01:37<01:56, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.59G/3.46G [01:38<01:56, 16.0MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.60G/3.46G [01:39<01:55, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.61G/3.46G [01:39<01:54, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.63G/3.46G [01:40<01:53, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 1.64G/3.46G [01:41<01:52, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.65G/3.46G [01:41<01:51, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.66G/3.46G [01:42<01:51, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.67G/3.46G [01:42<01:50, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.68G/3.46G [01:43<01:49, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.69G/3.46G [01:44<01:49, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1.70G/3.46G [01:44<01:48, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1.71G/3.46G [01:45<01:48, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1.72G/3.46G [01:46<01:47, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1.73G/3.46G [01:46<01:46, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.74G/3.46G [01:47<01:46, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.75G/3.46G [01:48<01:45, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.76G/3.46G [01:48<01:44, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1.77G/3.46G [01:49<01:44, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.78G/3.46G [01:50<01:43, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.79G/3.46G [01:50<01:43, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.80G/3.46G [01:51<01:42, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.81G/3.46G [01:52<01:42, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.82G/3.46G [01:52<01:41, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.84G/3.46G [01:53<01:40, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.85G/3.46G [01:53<01:39, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.86G/3.46G [01:54<01:39, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.87G/3.46G [01:55<01:38, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.88G/3.46G [01:55<01:37, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.89G/3.46G [01:56<01:37, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.90G/3.46G [01:57<01:36, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.91G/3.46G [01:57<01:36, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.92G/3.46G [01:58<01:35, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.93G/3.46G [01:59<01:34, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.94G/3.46G [01:59<01:34, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1.95G/3.46G [02:00<01:33, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1.96G/3.46G [02:01<01:32, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1.97G/3.46G [02:01<01:32, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1.98G/3.46G [02:02<01:31, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1.99G/3.46G [02:03<01:30, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.00G/3.46G [02:03<01:30, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.01G/3.46G [02:04<01:29, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.02G/3.46G [02:04<01:28, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2.03G/3.46G [02:05<01:27, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.04G/3.46G [02:06<01:27, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.06G/3.46G [02:06<01:26, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.07G/3.46G [02:07<01:26, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 2.08G/3.46G [02:08<01:25, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.09G/3.46G [02:08<01:25, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.10G/3.46G [02:09<01:24, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.11G/3.46G [02:10<01:23, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2.12G/3.46G [02:10<01:22, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.13G/3.46G [02:11<01:22, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.14G/3.46G [02:12<01:21, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.15G/3.46G [02:12<01:20, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.16G/3.46G [02:13<01:20, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.17G/3.46G [02:14<01:19, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.18G/3.46G [02:14<01:19, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.19G/3.46G [02:15<01:18, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.20G/3.46G [02:15<01:17, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.21G/3.46G [02:16<01:16, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.22G/3.46G [02:17<01:16, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.23G/3.46G [02:17<01:16, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2.24G/3.46G [02:18<01:15, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.25G/3.46G [02:19<01:14, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.26G/3.46G [02:19<01:13, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.28G/3.46G [02:20<01:14, 16.0MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2.29G/3.46G [02:21<01:13, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.30G/3.46G [02:21<01:12, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.31G/3.46G [02:22<01:11, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.32G/3.46G [02:23<01:11, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2.33G/3.46G [02:23<01:10, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.34G/3.46G [02:24<01:09, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.35G/3.46G [02:25<01:08, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.36G/3.46G [02:25<01:08, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.37G/3.46G [02:26<01:10, 15.6MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2.38G/3.46G [02:27<01:09, 15.6MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.39G/3.46G [02:27<01:08, 15.7MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.40G/3.46G [02:28<01:09, 15.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.41G/3.46G [02:29<01:05, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2.42G/3.46G [02:29<01:04, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2.43G/3.46G [02:30<01:03, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2.44G/3.46G [02:30<01:03, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2.45G/3.46G [02:31<01:02, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2.46G/3.46G [02:32<01:01, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.47G/3.46G [02:32<01:01, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.49G/3.46G [02:33<01:00, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.50G/3.46G [02:34<00:59, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.51G/3.46G [02:34<00:59, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.52G/3.46G [02:35<00:58, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.53G/3.46G [02:36<00:58, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.54G/3.46G [02:36<00:57, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.55G/3.46G [02:37<00:56, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.56G/3.46G [02:38<00:55, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.57G/3.46G [02:38<00:55, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.58G/3.46G [02:39<00:54, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2.59G/3.46G [02:40<00:53, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2.60G/3.46G [02:40<00:53, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2.61G/3.46G [02:41<00:52, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2.62G/3.46G [02:41<00:51, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2.63G/3.46G [02:42<00:51, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2.64G/3.46G [02:43<00:50, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2.65G/3.46G [02:43<00:49, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2.66G/3.46G [02:44<00:49, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2.67G/3.46G [02:45<00:54, 14.6MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2.68G/3.46G [02:45<00:46, 16.7MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2.69G/3.46G [02:46<00:46, 16.5MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2.71G/3.46G [02:47<00:46, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2.72G/3.46G [02:47<00:45, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2.73G/3.46G [02:48<00:45, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2.74G/3.46G [02:49<00:44, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2.75G/3.46G [02:49<00:44, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2.76G/3.46G [02:50<00:43, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 2.77G/3.46G [02:51<00:42, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2.78G/3.46G [02:51<00:42, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2.79G/3.46G [02:52<00:41, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2.80G/3.46G [02:53<00:40, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2.81G/3.46G [02:53<00:40, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.82G/3.46G [02:54<00:39, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.83G/3.46G [02:54<00:39, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.84G/3.46G [02:55<00:38, 16.0MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.85G/3.46G [02:56<00:38, 16.1MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.86G/3.46G [02:56<00:37, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.87G/3.46G [02:57<00:36, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.88G/3.46G [02:58<00:35, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.89G/3.46G [02:58<00:35, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.90G/3.46G [02:59<00:34, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.92G/3.46G [03:00<00:33, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.93G/3.46G [03:00<00:33, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2.94G/3.46G [03:01<00:32, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2.95G/3.46G [03:02<00:31, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2.96G/3.46G [03:02<00:31, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2.97G/3.46G [03:03<00:30, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2.98G/3.46G [03:04<00:29, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2.99G/3.46G [03:04<00:29, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.00G/3.46G [03:05<00:28, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.01G/3.46G [03:05<00:28, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.02G/3.46G [03:06<00:27, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 3.03G/3.46G [03:07<00:26, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3.04G/3.46G [03:07<00:26, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3.05G/3.46G [03:08<00:25, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3.06G/3.46G [03:09<00:24, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3.07G/3.46G [03:09<00:24, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3.08G/3.46G [03:10<00:23, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3.09G/3.46G [03:11<00:22, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3.10G/3.46G [03:11<00:22, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 3.11G/3.46G [03:12<00:21, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3.12G/3.46G [03:13<00:20, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3.14G/3.46G [03:13<00:20, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3.15G/3.46G [03:14<00:19, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3.16G/3.46G [03:15<00:18, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.17G/3.46G [03:15<00:18, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.18G/3.46G [03:16<00:17, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.19G/3.46G [03:16<00:17, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.20G/3.46G [03:17<00:16, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.21G/3.46G [03:18<00:15, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.22G/3.46G [03:18<00:15, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.23G/3.46G [03:19<00:14, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.24G/3.46G [03:20<00:13, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.25G/3.46G [03:20<00:13, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.26G/3.46G [03:21<00:12, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.27G/3.46G [03:22<00:11, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3.28G/3.46G [03:22<00:11, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.29G/3.46G [03:23<00:10, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.30G/3.46G [03:24<00:09, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.31G/3.46G [03:24<00:09, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3.32G/3.46G [03:25<00:09, 15.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.33G/3.46G [03:26<00:07, 16.6MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.34G/3.46G [03:26<00:07, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.36G/3.46G [03:27<00:06, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.37G/3.46G [03:27<00:05, 16.4MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 3.38G/3.46G [03:28<00:05, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.39G/3.46G [03:29<00:04, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.40G/3.46G [03:29<00:04, 16.3MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.41G/3.46G [03:30<00:03, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3.42G/3.46G [03:31<00:02, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.43G/3.46G [03:31<00:02, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.44G/3.46G [03:32<00:01, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.45G/3.46G [03:33<00:00, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3.46G/3.46G [03:33<00:00, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.46G/3.46G [03:33<00:00, 16.2MB/s]\rDownloading (â€¦)on_pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.46G/3.46G [03:33<00:00, 16.2MB/s]\n",
            "{'conv_out_kernel', 'class_embed_type', 'mid_block_only_cross_attention', 'addition_embed_type_num_heads', 'resnet_out_scale_factor', 'projection_class_embeddings_input_dim', 'timestep_post_act', 'conv_in_kernel', 'mid_block_type', 'cross_attention_norm', 'encoder_hid_dim', 'class_embeddings_concat', 'time_cond_proj_dim', 'time_embedding_type', 'time_embedding_dim', 'resnet_skip_time_act', 'time_embedding_act_fn', 'resnet_time_scale_shift', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
            "\rDownloading metadata:   0%|          | 0.00/731 [00:00<?, ?B/s]\rDownloading metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 731/731 [00:00<00:00, 615kB/s]\n",
            "\rDownloading readme:   0%|          | 0.00/1.80k [00:00<?, ?B/s]\rDownloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.80k/1.80k [00:00<00:00, 1.55MB/s]\n",
            "\rDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "\rDownloading data:   0%|          | 0.00/99.7M [00:00<?, ?B/s]\u001b[A\n",
            "\rDownloading data:   1%|          | 507k/99.7M [00:00<00:19, 5.05MB/s]\u001b[A\n",
            "\rDownloading data:   3%|â–         | 2.82M/99.7M [00:00<00:06, 15.7MB/s]\u001b[A\n",
            "\rDownloading data:   5%|â–         | 4.59M/99.7M [00:00<00:06, 15.7MB/s]\u001b[A\n",
            "\rDownloading data:   6%|â–‹         | 6.42M/99.7M [00:00<00:05, 15.9MB/s]\u001b[A\n",
            "\rDownloading data:   8%|â–Š         | 8.26M/99.7M [00:00<00:05, 16.0MB/s]\u001b[A\n",
            "\rDownloading data:  10%|â–ˆ         | 10.1M/99.7M [00:00<00:05, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  12%|â–ˆâ–        | 11.9M/99.7M [00:00<00:05, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  14%|â–ˆâ–        | 13.8M/99.7M [00:00<00:05, 16.8MB/s]\u001b[A\n",
            "\rDownloading data:  15%|â–ˆâ–Œ        | 15.4M/99.7M [00:00<00:05, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  17%|â–ˆâ–‹        | 17.2M/99.7M [00:01<00:05, 15.9MB/s]\u001b[A\n",
            "\rDownloading data:  19%|â–ˆâ–‰        | 19.0M/99.7M [00:01<00:05, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  21%|â–ˆâ–ˆ        | 20.8M/99.7M [00:01<00:04, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  23%|â–ˆâ–ˆâ–       | 22.7M/99.7M [00:01<00:04, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  25%|â–ˆâ–ˆâ–       | 24.5M/99.7M [00:01<00:04, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  26%|â–ˆâ–ˆâ–‹       | 26.3M/99.7M [00:01<00:04, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  28%|â–ˆâ–ˆâ–Š       | 28.1M/99.7M [00:01<00:04, 16.7MB/s]\u001b[A\n",
            "\rDownloading data:  30%|â–ˆâ–ˆâ–‰       | 29.8M/99.7M [00:01<00:04, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  32%|â–ˆâ–ˆâ–ˆâ–      | 31.6M/99.7M [00:01<00:04, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  34%|â–ˆâ–ˆâ–ˆâ–      | 33.4M/99.7M [00:02<00:04, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35.2M/99.7M [00:02<00:03, 16.5MB/s]\u001b[A\n",
            "\rDownloading data:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 36.8M/99.7M [00:02<00:03, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 38.7M/99.7M [00:02<00:03, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40.5M/99.7M [00:02<00:03, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42.3M/99.7M [00:02<00:03, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44.2M/99.7M [00:02<00:03, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46.0M/99.7M [00:02<00:03, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 47.8M/99.7M [00:02<00:03, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49.7M/99.7M [00:03<00:03, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 51.5M/99.7M [00:03<00:02, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53.3M/99.7M [00:03<00:02, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55.2M/99.7M [00:03<00:02, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57.0M/99.7M [00:03<00:02, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 58.9M/99.7M [00:03<00:02, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60.6M/99.7M [00:03<00:02, 16.7MB/s]\u001b[A\n",
            "\rDownloading data:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62.3M/99.7M [00:03<00:02, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64.1M/99.7M [00:03<00:02, 16.5MB/s]\u001b[A\n",
            "\rDownloading data:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65.7M/99.7M [00:04<00:02, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 67.5M/99.7M [00:04<00:02, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69.1M/99.7M [00:04<00:01, 16.0MB/s]\u001b[A\n",
            "\rDownloading data:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70.9M/99.7M [00:04<00:01, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72.6M/99.7M [00:04<00:01, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74.3M/99.7M [00:04<00:01, 16.0MB/s]\u001b[A\n",
            "\rDownloading data:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 76.0M/99.7M [00:04<00:01, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 77.7M/99.7M [00:04<00:01, 16.0MB/s]\u001b[A\n",
            "\rDownloading data:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79.6M/99.7M [00:04<00:01, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 81.4M/99.7M [00:05<00:01, 16.1MB/s]\u001b[A\n",
            "\rDownloading data:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83.1M/99.7M [00:05<00:01, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84.7M/99.7M [00:05<00:00, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 86.4M/99.7M [00:05<00:00, 15.9MB/s]\u001b[A\n",
            "\rDownloading data:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88.1M/99.7M [00:05<00:00, 16.2MB/s]\u001b[A\n",
            "\rDownloading data:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 89.8M/99.7M [00:05<00:00, 15.9MB/s]\u001b[A\n",
            "\rDownloading data:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 91.6M/99.7M [00:05<00:00, 16.5MB/s]\u001b[A\n",
            "\rDownloading data:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 93.2M/99.7M [00:05<00:00, 15.6MB/s]\u001b[A\n",
            "\rDownloading data:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95.0M/99.7M [00:05<00:00, 15.7MB/s]\u001b[A\n",
            "\rDownloading data:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 96.8M/99.7M [00:06<00:00, 16.3MB/s]\u001b[A\n",
            "\rDownloading data:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 98.4M/99.7M [00:06<00:00, 15.9MB/s]\u001b[A\rDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99.7M/99.7M [00:06<00:00, 16.1MB/s]\n",
            "\rDownloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.78s/it]\rDownloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.78s/it]\n",
            "\rExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\rExtracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1456.36it/s]\n",
            "\rGenerating train split:   0%|          | 0/833 [00:00<?, ? examples/s]\rGenerating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 833/833 [00:00<00:00, 1573.79 examples/s]\r                                                                                  \r\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 125.06it/s]\n",
            "wandb: Currently logged in as: jainr3 (rahul96jain). Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.15.2\n",
            "wandb: Run data is saved locally in /content/diffusers/examples/text_to_image/wandb/run-20230509_235657-abk3sp8b\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run skilled-terrain-2\n",
            "wandb: â­ï¸ View project at https://wandb.ai/rahul96jain/text2image-fine-tune\n",
            "wandb: ğŸš€ View run at https://wandb.ai/rahul96jain/text2image-fine-tune/runs/abk3sp8b\n",
            "05/09/2023 23:56:57 - INFO - __main__ - ***** Running training *****\n",
            "05/09/2023 23:56:57 - INFO - __main__ -   Num examples = 833\n",
            "05/09/2023 23:56:57 - INFO - __main__ -   Num Epochs = 100\n",
            "05/09/2023 23:56:57 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "05/09/2023 23:56:57 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "05/09/2023 23:56:57 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "05/09/2023 23:56:57 - INFO - __main__ -   Total optimization steps = 83300\n",
            "\r  0%|          | 0/83300 [00:00<?, ?it/s]\rSteps:   0%|          | 0/83300 [00:00<?, ?it/s]â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
            "â”‚ /content/diffusers/examples/text_to_image/train_text_to_image_lora.py:861 in â”‚\n",
            "â”‚ <module>                                                                     â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   858                                                                        â”‚\n",
            "â”‚   859                                                                        â”‚\n",
            "â”‚   860 if __name__ == \"__main__\":                                             â”‚\n",
            "â”‚ â± 861 â”‚   main()                                                             â”‚\n",
            "â”‚   862                                                                        â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /content/diffusers/examples/text_to_image/train_text_to_image_lora.py:729 in â”‚\n",
            "â”‚ main                                                                         â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   726 â”‚   â”‚   â”‚   â”‚   â”‚   raise ValueError(f\"Unknown prediction type {noise_ â”‚\n",
            "â”‚   727 â”‚   â”‚   â”‚   â”‚                                                          â”‚\n",
            "â”‚   728 â”‚   â”‚   â”‚   â”‚   # Predict the noise residual and compute loss          â”‚\n",
            "â”‚ â± 729 â”‚   â”‚   â”‚   â”‚   model_pred = unet(noisy_latents, timesteps, encoder_hi â”‚\n",
            "â”‚   730 â”‚   â”‚   â”‚   â”‚   loss = F.mse_loss(model_pred.float(), target.float(),  â”‚\n",
            "â”‚   731 â”‚   â”‚   â”‚   â”‚                                                          â”‚\n",
            "â”‚   732 â”‚   â”‚   â”‚   â”‚   # Gather the losses across all processes for logging ( â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501 in   â”‚\n",
            "â”‚ _call_impl                                                                   â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   1498 â”‚   â”‚   if not (self._backward_hooks or self._backward_pre_hooks or s â”‚\n",
            "â”‚   1499 â”‚   â”‚   â”‚   â”‚   or _global_backward_pre_hooks or _global_backward_hoo â”‚\n",
            "â”‚   1500 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚\n",
            "â”‚ â± 1501 â”‚   â”‚   â”‚   return forward_call(*args, **kwargs)                      â”‚\n",
            "â”‚   1502 â”‚   â”‚   # Do not call functions when jit is used                      â”‚\n",
            "â”‚   1503 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚\n",
            "â”‚   1504 â”‚   â”‚   backward_pre_hooks = []                                       â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/diffusers/models/unet_2d_condition.p â”‚\n",
            "â”‚ y:773 in forward                                                             â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   770 â”‚   â”‚   â”‚   â”‚   upsample_size = down_block_res_samples[-1].shape[2:]   â”‚\n",
            "â”‚   771 â”‚   â”‚   â”‚                                                              â”‚\n",
            "â”‚   772 â”‚   â”‚   â”‚   if hasattr(upsample_block, \"has_cross_attention\") and upsa â”‚\n",
            "â”‚ â± 773 â”‚   â”‚   â”‚   â”‚   sample = upsample_block(                               â”‚\n",
            "â”‚   774 â”‚   â”‚   â”‚   â”‚   â”‚   hidden_states=sample,                              â”‚\n",
            "â”‚   775 â”‚   â”‚   â”‚   â”‚   â”‚   temb=emb,                                          â”‚\n",
            "â”‚   776 â”‚   â”‚   â”‚   â”‚   â”‚   res_hidden_states_tuple=res_samples,               â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501 in   â”‚\n",
            "â”‚ _call_impl                                                                   â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   1498 â”‚   â”‚   if not (self._backward_hooks or self._backward_pre_hooks or s â”‚\n",
            "â”‚   1499 â”‚   â”‚   â”‚   â”‚   or _global_backward_pre_hooks or _global_backward_hoo â”‚\n",
            "â”‚   1500 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚\n",
            "â”‚ â± 1501 â”‚   â”‚   â”‚   return forward_call(*args, **kwargs)                      â”‚\n",
            "â”‚   1502 â”‚   â”‚   # Do not call functions when jit is used                      â”‚\n",
            "â”‚   1503 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚\n",
            "â”‚   1504 â”‚   â”‚   backward_pre_hooks = []                                       â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/diffusers/models/unet_2d_blocks.py:1 â”‚\n",
            "â”‚ 860 in forward                                                               â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   1857 â”‚   â”‚   â”‚   â”‚   )[0]                                                  â”‚\n",
            "â”‚   1858 â”‚   â”‚   â”‚   else:                                                     â”‚\n",
            "â”‚   1859 â”‚   â”‚   â”‚   â”‚   hidden_states = resnet(hidden_states, temb)           â”‚\n",
            "â”‚ â± 1860 â”‚   â”‚   â”‚   â”‚   hidden_states = attn(                                 â”‚\n",
            "â”‚   1861 â”‚   â”‚   â”‚   â”‚   â”‚   hidden_states,                                    â”‚\n",
            "â”‚   1862 â”‚   â”‚   â”‚   â”‚   â”‚   encoder_hidden_states=encoder_hidden_states,      â”‚\n",
            "â”‚   1863 â”‚   â”‚   â”‚   â”‚   â”‚   cross_attention_kwargs=cross_attention_kwargs,    â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501 in   â”‚\n",
            "â”‚ _call_impl                                                                   â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   1498 â”‚   â”‚   if not (self._backward_hooks or self._backward_pre_hooks or s â”‚\n",
            "â”‚   1499 â”‚   â”‚   â”‚   â”‚   or _global_backward_pre_hooks or _global_backward_hoo â”‚\n",
            "â”‚   1500 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚\n",
            "â”‚ â± 1501 â”‚   â”‚   â”‚   return forward_call(*args, **kwargs)                      â”‚\n",
            "â”‚   1502 â”‚   â”‚   # Do not call functions when jit is used                      â”‚\n",
            "â”‚   1503 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚\n",
            "â”‚   1504 â”‚   â”‚   backward_pre_hooks = []                                       â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/diffusers/models/transformer_2d.py:2 â”‚\n",
            "â”‚ 65 in forward                                                                â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   262 â”‚   â”‚                                                                  â”‚\n",
            "â”‚   263 â”‚   â”‚   # 2. Blocks                                                    â”‚\n",
            "â”‚   264 â”‚   â”‚   for block in self.transformer_blocks:                          â”‚\n",
            "â”‚ â± 265 â”‚   â”‚   â”‚   hidden_states = block(                                     â”‚\n",
            "â”‚   266 â”‚   â”‚   â”‚   â”‚   hidden_states,                                         â”‚\n",
            "â”‚   267 â”‚   â”‚   â”‚   â”‚   encoder_hidden_states=encoder_hidden_states,           â”‚\n",
            "â”‚   268 â”‚   â”‚   â”‚   â”‚   timestep=timestep,                                     â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501 in   â”‚\n",
            "â”‚ _call_impl                                                                   â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   1498 â”‚   â”‚   if not (self._backward_hooks or self._backward_pre_hooks or s â”‚\n",
            "â”‚   1499 â”‚   â”‚   â”‚   â”‚   or _global_backward_pre_hooks or _global_backward_hoo â”‚\n",
            "â”‚   1500 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚\n",
            "â”‚ â± 1501 â”‚   â”‚   â”‚   return forward_call(*args, **kwargs)                      â”‚\n",
            "â”‚   1502 â”‚   â”‚   # Do not call functions when jit is used                      â”‚\n",
            "â”‚   1503 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚\n",
            "â”‚   1504 â”‚   â”‚   backward_pre_hooks = []                                       â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/diffusers/models/attention.py:315 in â”‚\n",
            "â”‚ forward                                                                      â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   312 â”‚   â”‚   â”‚   norm_hidden_states = self.norm1(hidden_states)             â”‚\n",
            "â”‚   313 â”‚   â”‚                                                                  â”‚\n",
            "â”‚   314 â”‚   â”‚   cross_attention_kwargs = cross_attention_kwargs if cross_atten â”‚\n",
            "â”‚ â± 315 â”‚   â”‚   attn_output = self.attn1(                                      â”‚\n",
            "â”‚   316 â”‚   â”‚   â”‚   norm_hidden_states,                                        â”‚\n",
            "â”‚   317 â”‚   â”‚   â”‚   encoder_hidden_states=encoder_hidden_states if self.only_c â”‚\n",
            "â”‚   318 â”‚   â”‚   â”‚   attention_mask=attention_mask,                             â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501 in   â”‚\n",
            "â”‚ _call_impl                                                                   â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   1498 â”‚   â”‚   if not (self._backward_hooks or self._backward_pre_hooks or s â”‚\n",
            "â”‚   1499 â”‚   â”‚   â”‚   â”‚   or _global_backward_pre_hooks or _global_backward_hoo â”‚\n",
            "â”‚   1500 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_pre_hooks â”‚\n",
            "â”‚ â± 1501 â”‚   â”‚   â”‚   return forward_call(*args, **kwargs)                      â”‚\n",
            "â”‚   1502 â”‚   â”‚   # Do not call functions when jit is used                      â”‚\n",
            "â”‚   1503 â”‚   â”‚   full_backward_hooks, non_full_backward_hooks = [], []         â”‚\n",
            "â”‚   1504 â”‚   â”‚   backward_pre_hooks = []                                       â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor â”‚\n",
            "â”‚ .py:268 in forward                                                           â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚    265 â”‚   â”‚   # The `Attention` class can call different attention processo â”‚\n",
            "â”‚    266 â”‚   â”‚   # here we simply pass along all tensors to the selected proce â”‚\n",
            "â”‚    267 â”‚   â”‚   # For standard processors that are defined here, `**cross_att â”‚\n",
            "â”‚ â±  268 â”‚   â”‚   return self.processor(                                        â”‚\n",
            "â”‚    269 â”‚   â”‚   â”‚   self,                                                     â”‚\n",
            "â”‚    270 â”‚   â”‚   â”‚   hidden_states,                                            â”‚\n",
            "â”‚    271 â”‚   â”‚   â”‚   encoder_hidden_states=encoder_hidden_states,              â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor â”‚\n",
            "â”‚ .py:475 in __call__                                                          â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚    472 â”‚   â”‚   key = attn.head_to_batch_dim(key)                             â”‚\n",
            "â”‚    473 â”‚   â”‚   value = attn.head_to_batch_dim(value)                         â”‚\n",
            "â”‚    474 â”‚   â”‚                                                                 â”‚\n",
            "â”‚ â±  475 â”‚   â”‚   attention_probs = attn.get_attention_scores(query, key, atten â”‚\n",
            "â”‚    476 â”‚   â”‚   hidden_states = torch.bmm(attention_probs, value)             â”‚\n",
            "â”‚    477 â”‚   â”‚   hidden_states = attn.batch_to_head_dim(hidden_states)         â”‚\n",
            "â”‚    478                                                                       â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor â”‚\n",
            "â”‚ .py:309 in get_attention_scores                                              â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚    306 â”‚   â”‚   â”‚   baddbmm_input = attention_mask                            â”‚\n",
            "â”‚    307 â”‚   â”‚   â”‚   beta = 1                                                  â”‚\n",
            "â”‚    308 â”‚   â”‚                                                                 â”‚\n",
            "â”‚ â±  309 â”‚   â”‚   attention_scores = torch.baddbmm(                             â”‚\n",
            "â”‚    310 â”‚   â”‚   â”‚   baddbmm_input,                                            â”‚\n",
            "â”‚    311 â”‚   â”‚   â”‚   query,                                                    â”‚\n",
            "â”‚    312 â”‚   â”‚   â”‚   key.transpose(-1, -2),                                    â”‚\n",
            "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
            "OutOfMemoryError: CUDA out of memory. Tried to allocate 1.58 GiB (GPU 0; 14.75 \n",
            "GiB total capacity; 12.10 GiB already allocated; 284.81 MiB free; 13.19 GiB \n",
            "reserved in total by PyTorch) If reserved memory is >> allocated memory try \n",
            "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory \n",
            "Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\rSteps:   0%|          | 0/83300 [00:09<?, ?it/s]\n",
            "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
            "â”‚ /usr/local/bin/accelerate:8 in <module>                                      â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   5 from accelerate.commands.accelerate_cli import main                      â”‚\n",
            "â”‚   6 if __name__ == '__main__':                                               â”‚\n",
            "â”‚   7 â”‚   sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])     â”‚\n",
            "â”‚ â± 8 â”‚   sys.exit(main())                                                     â”‚\n",
            "â”‚   9                                                                          â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.p â”‚\n",
            "â”‚ y:45 in main                                                                 â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   42 â”‚   â”‚   exit(1)                                                         â”‚\n",
            "â”‚   43 â”‚                                                                       â”‚\n",
            "â”‚   44 â”‚   # Run                                                               â”‚\n",
            "â”‚ â± 45 â”‚   args.func(args)                                                     â”‚\n",
            "â”‚   46                                                                         â”‚\n",
            "â”‚   47                                                                         â”‚\n",
            "â”‚   48 if __name__ == \"__main__\":                                              â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py:918 in â”‚\n",
            "â”‚ launch_command                                                               â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   915 â”‚   elif defaults is not None and defaults.compute_environment == Comp â”‚\n",
            "â”‚   916 â”‚   â”‚   sagemaker_launcher(defaults, args)                             â”‚\n",
            "â”‚   917 â”‚   else:                                                              â”‚\n",
            "â”‚ â± 918 â”‚   â”‚   simple_launcher(args)                                          â”‚\n",
            "â”‚   919                                                                        â”‚\n",
            "â”‚   920                                                                        â”‚\n",
            "â”‚   921 def main():                                                            â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚ /usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py:580 in â”‚\n",
            "â”‚ simple_launcher                                                              â”‚\n",
            "â”‚                                                                              â”‚\n",
            "â”‚   577 â”‚   process.wait()                                                     â”‚\n",
            "â”‚   578 â”‚   if process.returncode != 0:                                        â”‚\n",
            "â”‚   579 â”‚   â”‚   if not args.quiet:                                             â”‚\n",
            "â”‚ â± 580 â”‚   â”‚   â”‚   raise subprocess.CalledProcessError(returncode=process.ret â”‚\n",
            "â”‚   581 â”‚   â”‚   else:                                                          â”‚\n",
            "â”‚   582 â”‚   â”‚   â”‚   sys.exit(1)                                                â”‚\n",
            "â”‚   583                                                                        â”‚\n",
            "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
            "CalledProcessError: Command '['/usr/bin/python3', 'train_text_to_image_lora.py',\n",
            "'--pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1', \n",
            "'--dataset_name=lambdalabs/pokemon-blip-captions', '--caption_column=text', \n",
            "'--resolution=768', '--random_flip', '--train_batch_size=1', \n",
            "'--num_train_epochs=100', '--checkpointing_steps=5000', '--learning_rate=1e-04',\n",
            "'--lr_scheduler=constant', '--lr_warmup_steps=0', '--seed=42', \n",
            "'--output_dir=sd-pokemon-model-lora', '--validation_prompt=cute dragon \n",
            "creature', '--report_to=wandb', '--push_to_hub']' returned non-zero exit status \n",
            "1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-73b5dd6319d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'accelerate launch --mixed_precision=\"fp16\" train_text_to_image_lora.py \\\\\\n  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" \\\\\\n  --dataset_name=\"lambdalabs/pokemon-blip-captions\" --caption_column=\"text\" \\\\\\n  --resolution=768 --random_flip \\\\\\n  --train_batch_size=1 \\\\\\n  --num_train_epochs=100 --checkpointing_steps=5000 \\\\\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\\\\n  --seed=42 \\\\\\n  --output_dir=\"sd-pokemon-model-lora\" \\\\\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\\\\n  --push_to_hub\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'accelerate launch --mixed_precision=\"fp16\" train_text_to_image_lora.py \\\\\\n  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" \\\\\\n  --dataset_name=\"lambdalabs/pokemon-blip-captions\" --caption_column=\"text\" \\\\\\n  --resolution=768 --random_flip \\\\\\n  --train_batch_size=1 \\\\\\n  --num_train_epochs=100 --checkpointing_steps=5000 \\\\\\n  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\\\\n  --seed=42 \\\\\\n  --output_dir=\"sd-pokemon-model-lora\" \\\\\\n  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\\\\n  --push_to_hub\\n'' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying to use diffusers library with xformers memory efficient attention"
      ],
      "metadata": {
        "id": "Wh6eKGASLDmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LORA training script\n",
        "# note: resolution is 768 for stable-diffusion-2-1 model\n",
        "# added enable_xformers_memory_efficient_attention parameter\n",
        "# 10-12 mins for 1 epoch using T4 High-Ram; used ~6-7 GPU RAM (run: jolly-paper-3)\n",
        "%%bash\n",
        "accelerate launch --mixed_precision=\"fp16\" train_text_to_image_lora.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1\" \\\n",
        "  --dataset_name=\"lambdalabs/pokemon-blip-captions\" --caption_column=\"text\" \\\n",
        "  --resolution=768 --random_flip \\\n",
        "  --train_batch_size=1 \\\n",
        "  --num_train_epochs=100 --checkpointing_steps=5000 \\\n",
        "  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
        "  --seed=42 \\\n",
        "  --output_dir=\"sd-pokemon-model-lora\" \\\n",
        "  --validation_prompt=\"cute dragon creature\" --report_to=\"wandb\" \\\n",
        "  --push_to_hub --enable_xformers_memory_efficient_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6onuaCfAT5Uo",
        "outputId": "45e8a579-931b-4d76-c3c7-2982a6d24486"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying to use PEFT + Dreambooth\n",
        "* Supposedly cuts it down to 8 GB"
      ],
      "metadata": {
        "id": "S5U7o2yUJEEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "w1nI3aWZJJnQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}